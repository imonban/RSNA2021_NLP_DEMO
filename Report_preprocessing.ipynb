{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "2efc3498-9be2-41f3-969d-feb71e1487d4",
   "metadata": {},
   "source": [
    "## Text radiology repors"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "id": "f7330799-ef05-477d-bd78-0acbcaffbcba",
   "metadata": {},
   "outputs": [],
   "source": [
    "raw_report = \"EXAM: Chest CT Angiography with IV contrast enhancement using pulmonary embolism protocol, including 3D image post processing.COMPARISON: Chest radiographs 2018/02. FINDINGS: Positive for acute pulmonary embolism. There are bilateral subsegmental pulmonary artery emboli. In the right lower lobe there is an occlusive subsegmental pulmonary artery emboli associated with a wedge-shaped pulmonary infarct. This was seen on the chest radiograph earlier today. IMPRESSION: 1. Positive for acute pulmonary embolism. 2. Wedge-shaped pulmonary infarct in the right lower lobe.\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "id": "f5765a8a-59af-4039-bded-1bf04dabd462",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'EXAM: Chest CT Angiography with IV contrast enhancement using pulmonary embolism protocol, including 3D image post processing.COMPARISON: Chest radiographs 2018/02. FINDINGS: Positive for acute pulmonary embolism. There are bilateral subsegmental pulmonary artery emboli. In the right lower lobe there is an occlusive subsegmental pulmonary artery emboli associated with a wedge-shaped pulmonary infarct. This was seen on the chest radiograph earlier today. IMPRESSION: 1. Positive for acute pulmonary embolism. 2. Wedge-shaped pulmonary infarct in the right lower lobe.'"
      ]
     },
     "execution_count": 20,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "raw_report"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "54686c33-d0fb-46ed-9bf8-2f4aeeda61b1",
   "metadata": {},
   "source": [
    "## Segmenting the sentences"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "id": "22603de3-984d-421e-924f-801815083117",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "['EXAM: Chest CT Angiography with IV contrast enhancement using pulmonary embolism protocol, including 3D image post processing.COMPARISON: Chest radiographs 2018/02.',\n",
       " 'FINDINGS: Positive for acute pulmonary embolism.',\n",
       " 'There are bilateral subsegmental pulmonary artery emboli.',\n",
       " 'In the right lower lobe there is an occlusive subsegmental pulmonary artery emboli associated with a wedge-shaped pulmonary infarct.',\n",
       " 'This was seen on the chest radiograph earlier today.',\n",
       " 'IMPRESSION: 1.',\n",
       " 'Positive for acute pulmonary embolism.',\n",
       " '2.',\n",
       " 'Wedge-shaped pulmonary infarct in the right lower lobe.']"
      ]
     },
     "execution_count": 17,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Tokenizing text into sentences\n",
    "from nltk.tokenize import sent_tokenize\n",
    "\n",
    "sentences = sent_tokenize(raw_report)\n",
    "sentences\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "421b64b9-6e87-4b7e-8bbf-a8c3323117c8",
   "metadata": {},
   "source": [
    "## Segmenting the sections"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "id": "284812f5-f02e-495e-9581-038ed49494e6",
   "metadata": {},
   "outputs": [],
   "source": [
    "## section segmentation\n",
    "import re\n",
    "def extract(txt):\n",
    "    txt = txt.encode(\"ascii\", errors=\"ignore\").decode()\n",
    "    txt = txt.lower()\n",
    "    txt = txt.replace('\\n', ' ')\n",
    "    txt = txt.replace('\\r', ' ')\n",
    "    txt = txt.replace('\\t', ' ')\n",
    "    re1 = '(\\\\()'  # Any Single Character 1\n",
    "    re2 = '.*?'  # Non-greedy match on filler\n",
    "    re3 = '(\\\\))'  # Any Single Character 2\n",
    "    rg = re.compile(re1 + re2 + re3, re.IGNORECASE | re.DOTALL)\n",
    "    tags = ['clinical indication: ','support devices: ','comparison: ','findings: ', 'impression:', 'critical findings', 'important findings']\n",
    "    sections = {'clinical indication: ': ' ','support devices: ': ' ','comparison: ': ' ','findings: ': ' ', 'impression: ': ' '}\n",
    "    for t in sections.keys():\n",
    "        try:\n",
    "            tmp = txt.split(t)[1]\n",
    "            for l in tags:\n",
    "                if t !=l:\n",
    "                    tmp = tmp.split(l)[0]\n",
    "                    sections[t] = re.sub(rg, ' ', tmp.split('these findings: ')[0])\n",
    "        except:\n",
    "            sections[t] = ' '\n",
    "    return sections"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "id": "40a6c020-508d-4058-a7e1-0f5bee1d6c7f",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "{'clinical indication: ': ' ',\n",
       " 'support devices: ': ' ',\n",
       " 'comparison: ': 'chest radiographs 2018/02. ',\n",
       " 'findings: ': 'positive for acute pulmonary embolism. there are bilateral subsegmental pulmonary artery emboli. in the right lower lobe there is an occlusive subsegmental pulmonary artery emboli associated with a wedge-shaped pulmonary infarct. this was seen on the chest radiograph earlier today. ',\n",
       " 'impression: ': '1. positive for acute pulmonary embolism. 2. wedge-shaped pulmonary infarct in the right lower lobe.'}"
      ]
     },
     "execution_count": 19,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "extract(raw_report)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "ddd84991-5e55-4c94-bec0-7dcb06e46a2f",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[['EXAM', ':', 'Chest', 'CT', 'Angiography', 'with', 'IV', 'contrast', 'enhancement', 'using', 'pulmonary', 'embolism', 'protocol', ',', 'including', '3D', 'image', 'post', 'processing.COMPARISON', ':', 'Chest', 'radiographs', '2018/02', '.'], ['FINDINGS', ':', 'Positive', 'for', 'acute', 'pulmonary', 'embolism', '.'], ['There', 'are', 'bilateral', 'subsegmental', 'pulmonary', 'artery', 'emboli', '.'], ['In', 'the', 'right', 'lower', 'lobe', 'there', 'is', 'an', 'occlusive', 'subsegmental', 'pulmonary', 'artery', 'emboli', 'associated', 'with', 'a', 'wedge-shaped', 'pulmonary', 'infarct', '.'], ['This', 'was', 'seen', 'on', 'the', 'chest', 'radiograph', 'earlier', 'today', '.'], ['IMPRESSION', ':', '1', '.'], ['Positive', 'for', 'acute', 'pulmonary', 'embolism', '.'], ['2', '.'], ['Wedge-shaped', 'pulmonary', 'infarct', 'in', 'the', 'right', 'lower', 'lobe', '.']]\n",
      "\n",
      "Number of words:91\n"
     ]
    }
   ],
   "source": [
    "# Tokenizing text into bags of words\n",
    "from nltk.tokenize import word_tokenize\n",
    "tokenized_docs = [word_tokenize(doc) for doc in sentences]\n",
    "print(tokenized_docs)\n",
    "\n",
    "## count number of words\n",
    "print('\\nNumber of words:'+str(sum( [len(listElem) for listElem in tokenized_docs])))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "id": "4a663e96-a199-4fac-8e93-8713f75d74a9",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[['EXAM', 'Chest', 'CT', 'Angiography', 'with', 'IV', 'contrast', 'enhancement', 'using', 'pulmonary', 'embolism', 'protocol', 'including', '3D', 'image', 'post', 'processingCOMPARISON', 'Chest', 'radiographs', '201802'], ['FINDINGS', 'Positive', 'for', 'acute', 'pulmonary', 'embolism'], ['There', 'are', 'bilateral', 'subsegmental', 'pulmonary', 'artery', 'emboli'], ['In', 'the', 'right', 'lower', 'lobe', 'there', 'is', 'an', 'occlusive', 'subsegmental', 'pulmonary', 'artery', 'emboli', 'associated', 'with', 'a', 'wedgeshaped', 'pulmonary', 'infarct'], ['This', 'was', 'seen', 'on', 'the', 'chest', 'radiograph', 'earlier', 'today'], ['IMPRESSION', '1'], ['Positive', 'for', 'acute', 'pulmonary', 'embolism'], ['2'], ['Wedgeshaped', 'pulmonary', 'infarct', 'in', 'the', 'right', 'lower', 'lobe']]\n",
      "\n",
      "Number of words after dropping punctuation:77\n"
     ]
    }
   ],
   "source": [
    "# Removing punctuation\n",
    "import re\n",
    "import string\n",
    "regex = re.compile('[%s]' % re.escape(string.punctuation)) #see documentation here: http://docs.python.org/2/library/string.html\n",
    "\n",
    "tokenized_docs_no_punctuation = []\n",
    "\n",
    "for review in tokenized_docs:\n",
    "    new_review = []\n",
    "    for token in review:\n",
    "        new_token = regex.sub(u'', token)\n",
    "        if not new_token == u'':\n",
    "            new_review.append(new_token)\n",
    "    \n",
    "    tokenized_docs_no_punctuation.append(new_review)\n",
    "    \n",
    "print(tokenized_docs_no_punctuation)\n",
    "\n",
    "## count number of words\n",
    "print('\\nNumber of words after dropping punctuation:'+str(sum( [len(listElem) for listElem in tokenized_docs_no_punctuation])))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "id": "0da48225-6072-4c9b-b1cc-d6501f314245",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[['EXAM', 'Chest', 'CT', 'Angiography', 'IV', 'contrast', 'enhancement', 'using', 'pulmonary', 'embolism', 'protocol', 'including', '3D', 'image', 'post', 'processingCOMPARISON', 'Chest', 'radiographs', '201802'], ['FINDINGS', 'Positive', 'acute', 'pulmonary', 'embolism'], ['There', 'bilateral', 'subsegmental', 'pulmonary', 'artery', 'emboli'], ['In', 'right', 'lower', 'lobe', 'occlusive', 'subsegmental', 'pulmonary', 'artery', 'emboli', 'associated', 'wedgeshaped', 'pulmonary', 'infarct'], ['This', 'seen', 'chest', 'radiograph', 'earlier', 'today'], ['IMPRESSION', '1'], ['Positive', 'acute', 'pulmonary', 'embolism'], ['2'], ['Wedgeshaped', 'pulmonary', 'infarct', 'right', 'lower', 'lobe']]\n",
      "\n",
      "Number of words after dropping stopwords:62\n"
     ]
    }
   ],
   "source": [
    "# Cleaning text of stopwords\n",
    "from nltk.corpus import stopwords\n",
    "\n",
    "tokenized_docs_no_stopwords = []\n",
    "\n",
    "for doc in tokenized_docs_no_punctuation:\n",
    "    new_term_vector = []\n",
    "    for word in doc:\n",
    "        if not word in stopwords.words('english'):\n",
    "            new_term_vector.append(word)\n",
    "    \n",
    "    tokenized_docs_no_stopwords.append(new_term_vector)\n",
    "\n",
    "print(tokenized_docs_no_stopwords)\n",
    "\n",
    "## count number of words\n",
    "print('\\nNumber of words after dropping stopwords:'+str(sum( [len(listElem) for listElem in tokenized_docs_no_stopwords])))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "id": "b274df35-021f-4f8d-a671-a2309fd78d97",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[['exam', 'chest', 'CT', 'angiographi', 'IV', 'contrast', 'enhanc', 'use', 'pulmonari', 'embol', 'protocol', 'includ', '3D', 'imag', 'post', 'processingcomparison', 'chest', 'radiograph', '201802'], ['find', 'posit', 'acut', 'pulmonari', 'embol'], ['there', 'bilater', 'subsegment', 'pulmonari', 'arteri', 'emboli'], ['In', 'right', 'lower', 'lobe', 'occlus', 'subsegment', 'pulmonari', 'arteri', 'emboli', 'associ', 'wedgeshap', 'pulmonari', 'infarct'], ['thi', 'seen', 'chest', 'radiograph', 'earlier', 'today'], ['impress', '1'], ['posit', 'acut', 'pulmonari', 'embol'], ['2'], ['wedgeshap', 'pulmonari', 'infarct', 'right', 'lower', 'lobe']]\n"
     ]
    }
   ],
   "source": [
    "# Stemming and Lemmatizing\n",
    "from nltk.stem.porter import PorterStemmer\n",
    "from nltk.stem.snowball import SnowballStemmer\n",
    "from nltk.stem.wordnet import WordNetLemmatizer\n",
    "\n",
    "porter = PorterStemmer()\n",
    "snowball = SnowballStemmer('english')\n",
    "wordnet = WordNetLemmatizer()\n",
    "\n",
    "preprocessed_docs = []\n",
    "\n",
    "for doc in tokenized_docs_no_stopwords:\n",
    "    final_doc = []\n",
    "    for word in doc:\n",
    "        final_doc.append(porter.stem(word))\n",
    "        #final_doc.append(snowball.stem(word))\n",
    "        #final_doc.append(wordnet.lemmatize(word))\n",
    "    \n",
    "    preprocessed_docs.append(final_doc)\n",
    "\n",
    "print(preprocessed_docs)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "66ef8d1f-390c-4dd1-87de-f1d141b14c54",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.12"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
